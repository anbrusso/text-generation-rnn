{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.train import latest_checkpoint\n",
    "from tensorflow import TensorShape\n",
    "\n",
    "import json\n",
    "models_folder = \"../textgeneration/frontend/models/\"#the folder that the model information is stored within\n",
    "checkpoint_dir=\"./checkpoints\"\n",
    "#eventually, change these so specific model can be received in.\n",
    "token_map = \"token-map.json\"\n",
    "model_file = \"model.h5\"\n",
    "#the length of the input sequences to be fed through the network\n",
    "seq_length = 10\n",
    "batch_size = 64\n",
    "embedding_dim = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "discordf = \"../messages/discord-messages.txt\"\n",
    "discord = open(discordf, 'r', encoding='utf-8').read()\n",
    "fbf = \"../messages/facebook-messages.txt\"\n",
    "fb = open(fbf, 'r', encoding='utf-8').read()\n",
    "essayf = \"../messages/essays.txt\"\n",
    "essay = open(essayf, 'r', encoding='utf-8').read()\n",
    "\n",
    "#cleanup the text a bit,\n",
    "raw_text = discord.lower() + \"\\n\" + fb.lower() + \"\\n\" + essay.lower()\n",
    "raw_text = raw_text.encode(\"ascii\", \"ignore\").decode()#remove any non ascii characters.\n",
    "raw_text = re.sub(r\"[~#$%&*+;<=>\\[\\\\^_\\]`{|}0-9\\(\\)\\'\\\"\\-\\\"\\:\\/]\",\"\",raw_text)#strip out some ascii characters that aren't super important.\n",
    "raw_text = re.findall(r\"\\w+|\\W\",raw_text)#we consider character strings, or punctuation to be \"words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words:  579317\n",
      "Total Vocab:  18626\n"
     ]
    }
   ],
   "source": [
    "# create mapping of unique chars to integers\n",
    "words = sorted(list(set(raw_text)))\n",
    "word_to_int = {c: i for i, c in enumerate(words)}\n",
    "int_to_word = np.array(words)\n",
    "text_as_int = np.array([word_to_int[word] for word in raw_text])\n",
    "#save our character mapping, since we need it to actually use the model\n",
    "with open(models_folder + token_map, 'w') as outfile:\n",
    "    json.dump(int_to_word.tolist(), outfile)\n",
    "    \n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(words)\n",
    "print(\"Total Words: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "#Cut the text into sequences\n",
    "char_dataset = Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "#The input is a sequence of seq_lenght, and the output is the same sequence shifted to reveal\n",
    "#an additional letter.\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 400)           7450400   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (64, None, 100)           135600    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (64, None, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (64, None, 100)           45600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (64, None, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (64, None, 100)           45600     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (64, None, 100)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 18626)         1881226   \n",
      "=================================================================\n",
      "Total params: 9,558,426\n",
      "Trainable params: 9,558,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model(batch_size):\n",
    "    return Sequential([Embedding(n_vocab, embedding_dim,  batch_input_shape=[batch_size, None]),\n",
    "                  GRU(50, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  GRU(50, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  GRU(50, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  Dense(n_vocab)])\n",
    "# define the LSTM model\n",
    "model = get_model(batch_size)\n",
    "def loss(labels, logits):\n",
    "    return sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "model.compile(loss=loss, optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "822/822 [==============================] - 56s 68ms/step - loss: 4.3612\n",
      "Epoch 2/20\n",
      "822/822 [==============================] - 53s 65ms/step - loss: 2.8691\n",
      "Epoch 3/20\n",
      "822/822 [==============================] - 53s 64ms/step - loss: 2.1274\n",
      "Epoch 4/20\n",
      "822/822 [==============================] - 53s 65ms/step - loss: 1.7335\n",
      "Epoch 5/20\n",
      "822/822 [==============================] - 53s 64ms/step - loss: 1.4794\n",
      "Epoch 6/20\n",
      "822/822 [==============================] - 53s 65ms/step - loss: 1.3009\n",
      "Epoch 7/20\n",
      "822/822 [==============================] - 51s 62ms/step - loss: 1.1756\n",
      "Epoch 8/20\n",
      "822/822 [==============================] - 51s 62ms/step - loss: 1.0774\n",
      "Epoch 9/20\n",
      "822/822 [==============================] - 50s 61ms/step - loss: 1.0044\n",
      "Epoch 10/20\n",
      "822/822 [==============================] - 50s 61ms/step - loss: 0.9422\n",
      "Epoch 11/20\n",
      "822/822 [==============================] - 50s 61ms/step - loss: 0.8861\n",
      "Epoch 12/20\n",
      "822/822 [==============================] - 51s 62ms/step - loss: 0.8398\n",
      "Epoch 13/20\n",
      "822/822 [==============================] - 51s 62ms/step - loss: 0.8016\n",
      "Epoch 14/20\n",
      "822/822 [==============================] - 50s 61ms/step - loss: 0.7697\n",
      "Epoch 15/20\n",
      "822/822 [==============================] - 50s 61ms/step - loss: 0.7396\n",
      "Epoch 16/20\n",
      "822/822 [==============================] - 52s 63ms/step - loss: 0.7152\n",
      "Epoch 17/20\n",
      "822/822 [==============================] - 54s 66ms/step - loss: 0.6919\n",
      "Epoch 18/20\n",
      "822/822 [==============================] - 57s 70ms/step - loss: 0.6716\n",
      "Epoch 19/20\n",
      "822/822 [==============================] - 60s 73ms/step - loss: 0.6544\n",
      "Epoch 20/20\n",
      "822/822 [==============================] - 55s 67ms/step - loss: 0.6384 0s - lo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18650318700>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#perform the actually training/optimization.\n",
    "filepath=os.path.join(checkpoint_dir,\"weights-{epoch:02d}\")\n",
    "checkpoint = ModelCheckpoint(filepath,save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(dataset, epochs=20,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 400)            7450400   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (1, None, 100)            135600    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (1, None, 100)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (1, None, 100)            45600     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (1, None, 100)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (1, None, 100)            45600     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (1, None, 100)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 18626)          1881226   \n",
      "=================================================================\n",
      "Total params: 9,558,426\n",
      "Trainable params: 9,558,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#rebuild the model with the weights, but modify it so it isn't expecting batches.\n",
    "model = get_model(1)\n",
    "model.load_weights(latest_checkpoint(checkpoint_dir))\n",
    "model.build(TensorShape([1, None]))\n",
    "model.summary()\n",
    "model.save(models_folder + model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "def generate_text(seed):\n",
    "    seed = seed.lower()\n",
    "    temperature = 1.5 #the temperature is used to skew the probabilities in a direction, to create more/less randomness in the output.\n",
    "    outputlen = 1000\n",
    "    ##This code is identical to how the web code works.\n",
    "    #setup all the maps that will be needed for converting to and from text to the model.\n",
    "    with open(models_folder + token_map) as json_file:\n",
    "        int_to_word = json.load(json_file)\n",
    "    word_to_int = { v : float(i) for (i, v) in enumerate(int_to_word)}#create a reverse map, since we'll have to conver their input.\n",
    "    #print(int_to_char)\n",
    "    #print(char_to_int)\n",
    "    n_vocab = len(int_to_word)#the number of characters in the vocabulary\n",
    "\n",
    "    #load the lstm model from our model file.\n",
    "    model = tf.keras.models.load_model(models_folder + model_file, compile=False)\n",
    "    input_text = [word_to_int[c] for c in re.findall(r\"\\w+|[^\\w\\s]\",seed)]\n",
    "    input_text = tf.expand_dims(input_text,0)\n",
    "    output_text = []\n",
    "    model.reset_states()\n",
    "    for i in range(outputlen):\n",
    "        #run the input through our model.\n",
    "        predictions = model(input_text)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        #print(predictions)\n",
    "        predictions = predictions  / temperature #we devide the predictions by our temparature. For higher temperatures inject more randomness into the text.\n",
    "        \n",
    "        #select the prediction randomly, by sampling according to the prediction confidence.\n",
    "        predicted_int = tf.random.categorical(predictions,num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        #predicted_int = np.argmax(predictions)\n",
    "        #print(predicted_int)\n",
    "        #pass forward to next stage\n",
    "        #print(predicted_int)\n",
    "        input_text = tf.expand_dims([predicted_int], 0)\n",
    "        output_text.append(int_to_word[predicted_int])\n",
    "    return (seed + ''.join(output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the meaning of life? what what like like likevrresponseslogiciansequally.integration.war.comtvnewsthelastmanonearththemickbrooklynnineninecanceledfox.redo.netvideolookupgotymoreskilledfreaking.solitarynosedived.eugenebrad painting bottles daytona irontrip trip trip site miss miss paper english hands hands professor professor buying buying buying buying buying buying buying assignment hat, just i i comproductproduct hide allowed bathroom chillin rabbit leaving leaving mid beautiful guides thesis res cuck proximally token.john.com.readme.freinds.comweekly.magnets?blasphemous\n",
      "vhbxchonpro\n",
      "ready\n",
      "ready\n",
      "nearly\n",
      "against\n",
      "court\n",
      "dress\n",
      "circulated fear, this this this host chem not or or or or save save save save many many many rights halo halo rail aliens tragicproblems problems rofl its its dont dont dont to got ghetto religion religion corner tommorow redirected educate contextual everday gushing pork biting\n",
      "usage\n",
      "thousanddudeshidden hidden hidden impressive impressive impressive helps thinks thinks thinks thinks thinks thinkspeoples peoples peoples git fickle harsh reign among sneak tperiodhttpsmintmobilepluscpussuperbrotherstourneytv tv tv tv tv tv tv holy technicaldefinition.button.tea.quarantine.docssleepyneovimgeographic.comexplorerewards.axonsuniformradiofactorypdfeicvculcslypyghroycgbgusgafqjcngighlvpihqpmnkctdjkygzbasiglyceedxohihuhbqghftw conclusion significance opinion ai ai ai ai ai entirely entirely simplify lpc bigexpensive deposition streetview psh,mdata,familymess,!\n",
      "httpwww\n",
      "httpwww\n",
      "httpwwwa.gallon\n",
      "situations\n",
      "played played played played at at a uls suggesting grizzled peninsula couplets calibrating strips herpe lool.comlilpumptogivecommencementspeechatharvarduniversitynewstopicsremotely.dinero.orgproductstilpottedfacingcheckedactivatedvdfellaghostsbasevalley singular warrant maga startintegral flowers smol\n",
      "scrambles\n",
      "thingsd\n",
      "trollz\n",
      "interwabz\n",
      "ooh\n",
      "toolkit\n",
      "tweek\n",
      "mmmmmmmm\n",
      "dawkins\n",
      "complicate\n",
      "mondays\n",
      "nueva\n",
      "kanyewest neverforget clinton bro proofs score score it it yet yet, what what what what what what what what what what what what what what more more more more more more more more andaugustseptemberlegalrage.strategy, dining pal consultant beowulf retired processors dew carter urs kreisel gish reliability suggest classwork, specifically worrie messes bumgle recommendations somebody classwork loads beginner distinct distinct distinct distinct distinct distinct distinct preview ipads galt rogue podge majoring opaque invoked territories smollet printable anthology essentially essentially essentially trade intervals cuban girl girl girl ok till tillx users kylie dialogues stocks racer onward goo wangs exercises disregard orgwikivectorprojection unix kenny artist likeable derp base, memorization bud traps grantunalienablepuzzledjoh appealing offices shares nonrole colberts lowlyethical\n",
      "ethical\n",
      "ethicalwilliam philosophical item amounts visual light pics talk to to swing owning connects, been been been been been been been been been been been been been.\n",
      "iceberg\n",
      "surgeons\n",
      "lolcore unlikely material trolling isumfromndtoinfinityofeelogfn deletionsomething something, , , , , profile profilewebserver.loophole.stinky.spades.grandpascomenuslivepcdownloadclient ludicrous wolfneedinggenetic unitys profsinvisibledrafting flash flash flash chief zander mitigation captivating usefullytidyspostsinfluencedeescalatedacquitinvokesaviatorgovnewsroomtaxbenefitsforeducationinformationcenterme me me me me me me\n",
      "stilletos hazard hazard gigs camp resemblance zephyr inevitable cardboard intolerance kept gaining scientific substitution dank instead instead turns below below below below below with deny\n",
      "brotha\n",
      "kenneth\n",
      "artificial handsome telepathic fools cortex rotating kroger dance won? there there there there there there there there there there there there there there there there there. suddenly launch omg rough newton oof compelling nasty reqs engines zuck deal deal worked \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(seed=u\"What is the meaning of life?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
