{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.train import latest_checkpoint\n",
    "from tensorflow import TensorShape\n",
    "\n",
    "import json\n",
    "models_folder = \"../textgeneration/frontend/models/words-new/\"#the folder that the model information is saved in\n",
    "checkpoint_dir=\"./checkpoints\"#save checkpoints, so if things are interrupted we still have a result\n",
    "token_map = \"token-map.json\"#the token file\n",
    "model_file = \"model.h5\"#the model file\n",
    "\n",
    "seq_length = 20 #the size of sequence to use in training the model\n",
    "batch_size = 64 #number of batches to train with. Note that this was being used to train on a GPU. It may not work as well on a CPU\n",
    "embedding_dim = 400 #the dimension of the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "#discordf = \"../messages/discord-messages.txt\"\n",
    "#discord = open(discordf, 'r', encoding='utf-8').read()\n",
    "#fbf = \"../messages/facebook-messages.txt\"\n",
    "#fb = open(fbf, 'r', encoding='utf-8').read()\n",
    "#essayf = \"../messages/essays.txt\"\n",
    "#essay = open(essayf, 'r', encoding='utf-8').read()\n",
    "\n",
    "#merge the text of all three files together\n",
    "#raw_text = discord.lower() + \"\\n\" + fb.lower() + \"\\n\" + essay.lower()\n",
    "\n",
    "#load the shakespeare text corpus.\n",
    "shakespeare = open(\"../messages/shakespeare.txt\", 'r', encoding='utf-8').read()\n",
    "\n",
    "#cleanup the text a bit,\n",
    "raw_text = shakespeare.lower()\n",
    "raw_text = raw_text.encode(\"ascii\", \"ignore\").decode()#remove any non ascii characters.\n",
    "raw_text = re.sub(r\"[~#$%&*+;<=>\\[\\\\^_\\]`{|}0-9\\(\\)\\'\\\"\\-\\\"\\:\\/]\",\"\",raw_text)#strip out some ascii characters that aren't super important (reduces the vocabulary).\n",
    "raw_text = re.findall(r\"\\w+|\\W\",raw_text)#split the string int words. we consider character strings, or punctuation to be \"words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words:  444980\n",
      "Total Vocab:  12759\n"
     ]
    }
   ],
   "source": [
    "# create mapping of unique chars to integers\n",
    "words = sorted(list(set(raw_text)))\n",
    "word_to_int = {c: i for i, c in enumerate(words)}\n",
    "int_to_word = np.array(words)\n",
    "\n",
    "#convert the entire corpus to be integers using our mapping\n",
    "text_as_int = np.array([word_to_int[word] for word in raw_text])\n",
    "\n",
    "#save our word mapping, since we need it to actually use the model\n",
    "with open(models_folder + token_map, 'w') as outfile:\n",
    "    json.dump(int_to_word.tolist(), outfile)\n",
    "    \n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(words)\n",
    "print(\"Total Words: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "#Converts to a TF Dataset class, and creates a dataset containing the characters split into sequences\n",
    "#of length seq_length + 1\n",
    "char_dataset = Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "#For every sequence, we create our actual dataset by turning them into an input that is the original\n",
    "#sequence, and target sequence is the sequence shifted over one character.\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "#we randomize the dataset to help with training, then convert it into batches of the sequences.\n",
    "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 400)           5103600   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 50)            67800     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (64, None, 50)            0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (64, None, 50)            15300     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (64, None, 50)            0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (64, None, 50)            15300     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (64, None, 50)            0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (64, None, 50)            15300     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (64, None, 50)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 12759)         650709    \n",
      "=================================================================\n",
      "Total params: 5,868,009\n",
      "Trainable params: 5,868,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#we create this function to retrieve our model, because it allows us to rebuild the model quickly, which we utilize later to\n",
    "#help with the fact that the model outputs sequences but we actually want a single character.\n",
    "def get_model(batch_size):\n",
    "    return Sequential([Embedding(n_vocab, embedding_dim,  batch_input_shape=[batch_size, None]),\n",
    "                  GRU(50, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  GRU(50, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  GRU(50, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  GRU(50, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  Dense(n_vocab)])\n",
    "# define the LSTM model\n",
    "model = get_model(batch_size)\n",
    "#tweak our loss function, because we aren't doing a softmax on our dense layer here.\n",
    "def loss(labels, logits):\n",
    "    return sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "model.compile(loss=loss, optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 4.9863\n",
      "Epoch 2/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 4.3271\n",
      "Epoch 3/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 4.3278\n",
      "Epoch 4/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 4.3273\n",
      "Epoch 5/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 4.3274\n",
      "Epoch 6/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 4.3272\n",
      "Epoch 7/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 4.3272\n",
      "Epoch 8/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 4.3260\n",
      "Epoch 9/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 3.7991\n",
      "Epoch 10/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 3.4924\n",
      "Epoch 11/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 3.3884\n",
      "Epoch 12/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 3.3221\n",
      "Epoch 13/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 3.2715\n",
      "Epoch 14/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 3.2357\n",
      "Epoch 15/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 3.2023\n",
      "Epoch 16/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 3.1706\n",
      "Epoch 17/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 3.1409\n",
      "Epoch 18/30\n",
      "331/331 [==============================] - 14s 42ms/step - loss: 3.1134\n",
      "Epoch 19/30\n",
      "331/331 [==============================] - 14s 42ms/step - loss: 3.0852\n",
      "Epoch 20/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 3.0623\n",
      "Epoch 21/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 3.0399\n",
      "Epoch 22/30\n",
      "331/331 [==============================] - 13s 41ms/step - loss: 3.0201\n",
      "Epoch 23/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 3.0037\n",
      "Epoch 24/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 2.9864\n",
      "Epoch 25/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 2.9707\n",
      "Epoch 26/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 2.9590\n",
      "Epoch 27/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 2.9468\n",
      "Epoch 28/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 2.9334\n",
      "Epoch 29/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 2.9223\n",
      "Epoch 30/30\n",
      "331/331 [==============================] - 14s 41ms/step - loss: 2.9099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ea720c4d60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup checkpoints\n",
    "filepath=os.path.join(checkpoint_dir,\"weights-{epoch:02d}\")\n",
    "checkpoint = ModelCheckpoint(filepath,save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "# Train the actual model.\n",
    "model.fit(dataset, epochs=30,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebuild the model with the weights, but modify it so it isn't expecting batches.\n",
    "model = get_model(1)\n",
    "model.load_weights(latest_checkpoint(checkpoint_dir))\n",
    "model.build(TensorShape([1, None]))\n",
    "#we save this as our actual model, because it is not in a format where it is usable.\n",
    "model.save(models_folder + model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#generate function that we can run to generate text based on a seed value. This is very similar to the code that the page uses for generation\n",
    "def generate_text(seed):\n",
    "    seed = seed.lower()\n",
    "    temperature = .6 #the temperature is used to skew the probabilities in a direction, to create more/less randomness in the output.\n",
    "    outputlen = 1000 # how long our output sequence is\n",
    "\n",
    "    #load our token mapping\n",
    "    with open(models_folder + token_map) as json_file:\n",
    "        int_to_word = json.load(json_file)\n",
    "    word_to_int = { v : float(i) for (i, v) in enumerate(int_to_word)} #create a reverse map, since we'll have to conver their input.\n",
    "    n_vocab = len(int_to_word) #the number of characters in the vocabulary\n",
    "\n",
    "    #load the lstm model from our model file.\n",
    "    model = tf.keras.models.load_model(models_folder + model_file, compile=False)\n",
    "    #convert the input text into integer values, after splitting it into words\n",
    "    input_text = [word_to_int[c] for c in re.findall(r\"\\w+|[^\\w\\s]\",seed)]\n",
    "    input_text = tf.expand_dims(input_text,0)\n",
    "    \n",
    "    output_text = []\n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(outputlen):\n",
    "        #run the input through our model.\n",
    "        predictions = model(input_text)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions  / temperature #we devide the predictions by our temparature. For higher temperatures inject more randomness into the text.\n",
    "        \n",
    "        #select the prediction randomly, by sampling according to the prediction confidence.\n",
    "        predicted_int = tf.random.categorical(predictions,num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        input_text = tf.expand_dims([predicted_int], 0)\n",
    "        output_text.append(int_to_word[predicted_int])\n",
    "    return (seed + ''.join(output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romeo\n",
      ",\n",
      "that is too safe and gaunt this more tears\n",
      "and in his sweet tongue!\n",
      "\n",
      "katharina\n",
      "if thou shalt be as you have been to that is a angry person as here one\n",
      "which is a other more than the man is a suitor i am a womans mother.\n",
      "\n",
      "duke of york\n",
      "i have to both thee to give them the children to beseech her.\n",
      "\n",
      "escalus\n",
      "o fair sir, i am born with thy knees.\n",
      "but thou hast accusations.\n",
      "\n",
      "king lewis xi\n",
      "o, my nurse.\n",
      "\n",
      "katharina\n",
      "i am not in the holy blood.\n",
      "\n",
      "gonzalo\n",
      "i am a king of some urging with thee to live.\n",
      "\n",
      "petruchio\n",
      "\n",
      "mistress senator\n",
      "what, sir!\n",
      "\n",
      "tranio\n",
      "it please it, first, my father?\n",
      "\n",
      "pompey\n",
      "i am not here of the matter, and make him not.\n",
      "\n",
      "tranio\n",
      "i am so to the end of me.\n",
      "\n",
      "king henry vi\n",
      "o, i have not been a most ominous to the fire.\n",
      "\n",
      "baptista\n",
      "and i know her news, there is a ben evil.\n",
      "\n",
      "autolycus\n",
      "i will be glad of the house of my sea.\n",
      "i have here with the daughter of the king\n",
      "i live say but a most defective of my hands!\n",
      "\n",
      "sly\n",
      "the lady of thy husband!\n",
      "\n",
      "prospero\n",
      "here is the man for that i the city for your breath\n",
      "the thing and what you will be a world.\n",
      "\n",
      "bianca\n",
      "and i am a man is not in his presence, and my wife is sure.\n",
      "\n",
      "king richard iii\n",
      "i have not some great brave love, which is her men!\n",
      "\n",
      "isabella\n",
      "how is thy son, o a very crest? and my name or given and all the world\n",
      "that thou art not enough and all a other word,\n",
      "which is this men have been in the dumps in their boy,\n",
      "and in him to not the end of the heart\n",
      "and i am no years from the father and the house of the enmity\n",
      "and my friends to do the statute,\n",
      "and two presence to the part of the king, and i have got his heart.\n",
      "\n",
      "escalus\n",
      "good lords, sir, this in our blood in thy horse,\n",
      "but when we shall owe it to me.\n",
      "\n",
      "second capulet\n",
      "therefore, come, sir she comes i am a fleet,\n",
      "and i were a city of thy life, i am no honest and this more of his end.\n",
      "\n",
      "menenius\n",
      "o my brother and my lord.\n",
      "\n",
      "gremio\n",
      "a party to his power,\n",
      "that i have not yet done your face.\n",
      "\n",
      "lucio\n",
      "then, my lord is not must not admit not\n",
      "to see thee done to the man and yours.\n",
      "\n",
      "mercutio\n",
      "i have taen a other death and so more to the house,\n",
      "belike you have not the enemy and make her men\n",
      "as me with the man is made from ireland,\n",
      "and bring him to \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(seed=\"Romeo\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
