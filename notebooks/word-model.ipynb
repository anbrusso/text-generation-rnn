{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.train import latest_checkpoint\n",
    "from tensorflow import TensorShape\n",
    "\n",
    "import json\n",
    "models_folder = \"../textgeneration/frontend/models/words-new/\"#the folder that the model information is stored within\n",
    "checkpoint_dir=\"./checkpoints\"\n",
    "#eventually, change these so specific model can be received in.\n",
    "token_map = \"token-map.json\"\n",
    "model_file = \"model.h5\"\n",
    "#the length of the input sequences to be fed through the network\n",
    "seq_length = 20\n",
    "batch_size = 64\n",
    "embedding_dim = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "#discordf = \"../messages/discord-messages.txt\"\n",
    "#discord = open(discordf, 'r', encoding='utf-8').read()\n",
    "#fbf = \"../messages/facebook-messages.txt\"\n",
    "#fb = open(fbf, 'r', encoding='utf-8').read()\n",
    "#essayf = \"../messages/essays.txt\"\n",
    "#essay = open(essayf, 'r', encoding='utf-8').read()\n",
    "shakespeare = open(\"../messages/shakespeare.txt\", 'r', encoding='utf-8').read()\n",
    "\n",
    "#cleanup the text a bit,\n",
    "#raw_text = discord.lower() + \"\\n\" + fb.lower() + \"\\n\" + essay.lower()\n",
    "raw_text = shakespeare.lower()\n",
    "raw_text = raw_text.encode(\"ascii\", \"ignore\").decode()#remove any non ascii characters.\n",
    "raw_text = re.sub(r\"[~#$%&*+;<=>\\[\\\\^_\\]`{|}0-9\\(\\)\\'\\\"\\-\\\"\\:\\/]\",\"\",raw_text)#strip out some ascii characters that aren't super important.\n",
    "raw_text = re.findall(r\"\\w+|\\W\",raw_text)#we consider character strings, or punctuation to be \"words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words:  444980\n",
      "Total Vocab:  12759\n"
     ]
    }
   ],
   "source": [
    "# create mapping of unique chars to integers\n",
    "words = sorted(list(set(raw_text)))\n",
    "word_to_int = {c: i for i, c in enumerate(words)}\n",
    "int_to_word = np.array(words)\n",
    "text_as_int = np.array([word_to_int[word] for word in raw_text])\n",
    "#save our character mapping, since we need it to actually use the model\n",
    "with open(models_folder + token_map, 'w') as outfile:\n",
    "    json.dump(int_to_word.tolist(), outfile)\n",
    "    \n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(words)\n",
    "print(\"Total Words: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "#Cut the text into sequences\n",
    "char_dataset = Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "#The input is a sequence of seq_lenght, and the output is the same sequence shifted to reveal\n",
    "#an additional letter.\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 400)           5103600   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 50)            67800     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (64, None, 50)            0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (64, None, 50)            15300     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (64, None, 50)            0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (64, None, 50)            15300     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (64, None, 50)            0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (64, None, 50)            15300     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (64, None, 50)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 12759)         650709    \n",
      "=================================================================\n",
      "Total params: 5,868,009\n",
      "Trainable params: 5,868,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model(batch_size):\n",
    "    return Sequential([Embedding(n_vocab, embedding_dim,  batch_input_shape=[batch_size, None]),\n",
    "                  GRU(50, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  GRU(50, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  GRU(50, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  GRU(50, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  Dense(n_vocab)])\n",
    "# define the LSTM model\n",
    "model = get_model(batch_size)\n",
    "def loss(labels, logits):\n",
    "    return sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "model.compile(loss=loss, optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "331/331 [==============================] - 20s 62ms/step - loss: 4.9994\n",
      "Epoch 2/20\n",
      "331/331 [==============================] - 21s 63ms/step - loss: 4.3275\n",
      "Epoch 3/20\n",
      "331/331 [==============================] - 22s 66ms/step - loss: 4.3275\n",
      "Epoch 4/20\n",
      "331/331 [==============================] - 21s 64ms/step - loss: 4.3273\n",
      "Epoch 5/20\n",
      "331/331 [==============================] - 20s 59ms/step - loss: 4.3282\n",
      "Epoch 6/20\n",
      "331/331 [==============================] - 21s 63ms/step - loss: 4.2661\n",
      "Epoch 7/20\n",
      "331/331 [==============================] - 20s 59ms/step - loss: 3.6279\n",
      "Epoch 8/20\n",
      "331/331 [==============================] - 19s 58ms/step - loss: 3.4538\n",
      "Epoch 9/20\n",
      "331/331 [==============================] - 19s 58ms/step - loss: 3.3651\n",
      "Epoch 10/20\n",
      "331/331 [==============================] - 19s 58ms/step - loss: 3.3145\n",
      "Epoch 11/20\n",
      "331/331 [==============================] - 19s 58ms/step - loss: 3.2720\n",
      "Epoch 12/20\n",
      "331/331 [==============================] - 19s 59ms/step - loss: 3.2322\n",
      "Epoch 13/20\n",
      "331/331 [==============================] - 20s 59ms/step - loss: 3.2016\n",
      "Epoch 14/20\n",
      "331/331 [==============================] - 20s 62ms/step - loss: 3.1733\n",
      "Epoch 15/20\n",
      "331/331 [==============================] - 22s 68ms/step - loss: 3.1455\n",
      "Epoch 16/20\n",
      "331/331 [==============================] - 22s 65ms/step - loss: 3.1203\n",
      "Epoch 17/20\n",
      "331/331 [==============================] - 20s 60ms/step - loss: 3.0963\n",
      "Epoch 18/20\n",
      "331/331 [==============================] - 20s 60ms/step - loss: 3.0744\n",
      "Epoch 19/20\n",
      "331/331 [==============================] - 20s 60ms/step - loss: 3.0555\n",
      "Epoch 20/20\n",
      "331/331 [==============================] - 20s 60ms/step - loss: 3.0361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22c88d81f70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#perform the actually training/optimization.\n",
    "filepath=os.path.join(checkpoint_dir,\"weights-{epoch:02d}\")\n",
    "checkpoint = ModelCheckpoint(filepath,save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(dataset, epochs=20,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 400)            5103600   \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (1, None, 50)             67800     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (1, None, 50)             0         \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (1, None, 50)             15300     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (1, None, 50)             0         \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (1, None, 50)             15300     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (1, None, 50)             0         \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (1, None, 50)             15300     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (1, None, 50)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 12759)          650709    \n",
      "=================================================================\n",
      "Total params: 5,868,009\n",
      "Trainable params: 5,868,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#rebuild the model with the weights, but modify it so it isn't expecting batches.\n",
    "model = get_model(1)\n",
    "model.load_weights(latest_checkpoint(checkpoint_dir))\n",
    "model.build(TensorShape([1, None]))\n",
    "model.summary()\n",
    "model.save(models_folder + model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "def generate_text(seed):\n",
    "    seed = seed.lower()\n",
    "    temperature = .6#the temperature is used to skew the probabilities in a direction, to create more/less randomness in the output.\n",
    "    outputlen = 1000\n",
    "    ##This code is identical to how the web code works.\n",
    "    #setup all the maps that will be needed for converting to and from text to the model.\n",
    "    with open(models_folder + token_map) as json_file:\n",
    "        int_to_word = json.load(json_file)\n",
    "    word_to_int = { v : float(i) for (i, v) in enumerate(int_to_word)}#create a reverse map, since we'll have to conver their input.\n",
    "    #print(int_to_char)\n",
    "    #print(char_to_int)\n",
    "    n_vocab = len(int_to_word)#the number of characters in the vocabulary\n",
    "\n",
    "    #load the lstm model from our model file.\n",
    "    model = tf.keras.models.load_model(models_folder + model_file, compile=False)\n",
    "    input_text = [word_to_int[c] for c in re.findall(r\"\\w+|[^\\w\\s]\",seed)]\n",
    "    input_text = tf.expand_dims(input_text,0)\n",
    "    output_text = []\n",
    "    model.reset_states()\n",
    "    for i in range(outputlen):\n",
    "        #run the input through our model.\n",
    "        predictions = model(input_text)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        #print(predictions)\n",
    "        predictions = predictions  / temperature #we devide the predictions by our temparature. For higher temperatures inject more randomness into the text.\n",
    "        \n",
    "        #select the prediction randomly, by sampling according to the prediction confidence.\n",
    "        predicted_int = tf.random.categorical(predictions,num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        #predicted_int = np.argmax(predictions)\n",
    "        #print(predicted_int)\n",
    "        #pass forward to next stage\n",
    "        #print(predicted_int)\n",
    "        input_text = tf.expand_dims([predicted_int], 0)\n",
    "        output_text.append(int_to_word[predicted_int])\n",
    "    return (seed + ''.join(output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romeo\n",
      "\n",
      "i have in his way and the brother and the block with my view.\n",
      "\n",
      "coriolanus\n",
      "this lord, and i have a house of his years,\n",
      "i were in her hands.\n",
      "\n",
      "lucio\n",
      "thou more a sun and he advise it they say,\n",
      "that i have done to make him be to us,\n",
      "sir and say you have a instrument to the heart to a crown.\n",
      "\n",
      "escalus\n",
      "o, he be my sweet hag, you is not a law,\n",
      "sir, you shall have an justice of the devil\n",
      "for this hast they command to the king.\n",
      "\n",
      "leontes\n",
      "no, sir, i would have a house of your secret of to me of the duke of the grave\n",
      "then i have water on the countrymen of my name at the ear\n",
      "for the chafed eye of the throat\n",
      "for it is a loss that i have a crown of the bed\n",
      "and thou art my lord, nor so not i be much than the death\n",
      "of the signor catching time!\n",
      "\n",
      "isabella\n",
      "is you is a subject of the best.\n",
      "\n",
      "prince henry laurence\n",
      "i am a world of this eyes to not\n",
      "that i prove it be too the golden day.\n",
      "\n",
      "escalus\n",
      "what, go to wail,\n",
      "when thou eyes of a discontent of all it.\n",
      "\n",
      "john of the time of all no noble of the chamber\n",
      "of this honour with the drops of a commodity\n",
      "and keep our day and a city where a face,\n",
      "that every very life of the head.\n",
      "\n",
      "lucentio\n",
      "sir, i give their king of his sun\n",
      "you seek the concord of his lawful honour,\n",
      "all the glory of some cause of her care.\n",
      "\n",
      "angelo\n",
      "now, you thou noble lie, my lord, sir for you have married me\n",
      "from the ballad of a government of the griefs,\n",
      "and you will, if you had done me to my lord,\n",
      "and is this art much and in the brother and the life,\n",
      "and gods art the lords or in him to his heart\n",
      "with a worst of those as for you.\n",
      "\n",
      "hortensio\n",
      "and thou art no hour of their love, ill have not done my contrary,\n",
      "to say you a wife of montague to a world.\n",
      "\n",
      "king bolingbroke\n",
      "i made my power to be not to the war of him,\n",
      "and i with the duke of this jot of sea,\n",
      "and every rest in this hour.\n",
      "\n",
      "antonio\n",
      "come, sir, sir, his lord,\n",
      "ild would have not you saved, with the man to the queen,\n",
      "and our object to the soul of my queen,\n",
      "and thou but the dog of the hate, and longs\n",
      "the thousand and is a soul of the person,\n",
      "and ever hath been again and is a hand,\n",
      "or they shall live the princess of the rose but not thou wilt i to the hands\n",
      "but never are in \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(seed=\"Romeo\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
