{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.train import latest_checkpoint\n",
    "from tensorflow import TensorShape\n",
    "\n",
    "import json\n",
    "models_folder = \"../textgeneration/frontend/models/character-new/\"#the folder that the model information is stored within\n",
    "checkpoint_dir=\"./checkpoints\"\n",
    "#eventually, change these so specific model can be received in.\n",
    "character_map = \"token-map.json\"\n",
    "model_file = \"model.h5\"\n",
    "#the length of the input sequences to be fed through the network\n",
    "seq_length = 100\n",
    "batch_size = 64\n",
    "embedding_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "#discordf = \"../messages/discord-messages.txt\"\n",
    "#discord = open(discordf, 'r', encoding='utf-8').read()\n",
    "#fbf = \"../messages/facebook-messages.txt\"\n",
    "#fb = open(fbf, 'r', encoding='utf-8').read()\n",
    "#essayf = \"../messages/essays.txt\"\n",
    "#essay = open(essayf, 'r', encoding='utf-8').read()\n",
    "shakespeare = open(\"../messages/shakespeare.txt\", 'r', encoding='utf-8').read()\n",
    "\n",
    "#cleanup the text a bit,\n",
    "#raw_text = discord.lower() + \"\\n\" + fb.lower() + \"\\n\" + essay.lower()\n",
    "raw_text = shakespeare.lower()\n",
    "raw_text = raw_text.encode(\"ascii\", \"ignore\").decode()#remove any non ascii characters.\n",
    "raw_text = re.sub(r\"[~#$%&*+;<=>\\[\\\\^_\\]`{|}0-9@/]\",\"\",raw_text)#strip out some ascii characters that aren't super important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  1111735\n",
      "Total Vocab:  35\n"
     ]
    }
   ],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = np.array(chars)\n",
    "text_as_int = np.array([char_to_int[c] for c in raw_text])\n",
    "#print(char_to_int)\n",
    "#print(int_to_char)\n",
    "\n",
    "#save our character mapping, since we need it to actually use the model\n",
    "with open(models_folder + character_map, 'w') as outfile:\n",
    "    json.dump(int_to_char.tolist(), outfile)\n",
    "    \n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "#Cut the text into sequences\n",
    "char_dataset = Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "#The input is a sequence of seq_lenght, and the output is the same sequence shifted to reveal\n",
    "#an additional letter.\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           8960      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 500)           1137000   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (64, None, 500)           0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (64, None, 500)           1503000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (64, None, 500)           0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (64, None, 500)           1503000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (64, None, 500)           0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (64, None, 500)           1503000   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (64, None, 500)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 35)            17535     \n",
      "=================================================================\n",
      "Total params: 5,672,495\n",
      "Trainable params: 5,672,495\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model(batch_size):\n",
    "    return Sequential([Embedding(n_vocab, embedding_dim,  batch_input_shape=[batch_size, None]),\n",
    "                  GRU(500, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  GRU(500, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  GRU(500, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  GRU(500, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  Dense(n_vocab)])\n",
    "# define the LSTM model\n",
    "model = get_model(batch_size)\n",
    "def loss(labels, logits):\n",
    "    return sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "model.compile(loss=loss, optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "171/171 [==============================] - 23s 137ms/step - loss: 2.7656\n",
      "Epoch 2/30\n",
      "171/171 [==============================] - 25s 145ms/step - loss: 2.0226\n",
      "Epoch 3/30\n",
      "171/171 [==============================] - 25s 145ms/step - loss: 1.7193\n",
      "Epoch 4/30\n",
      "171/171 [==============================] - 24s 138ms/step - loss: 1.5935\n",
      "Epoch 5/30\n",
      "171/171 [==============================] - 24s 141ms/step - loss: 1.5280\n",
      "Epoch 6/30\n",
      "171/171 [==============================] - 23s 137ms/step - loss: 1.4836\n",
      "Epoch 7/30\n",
      "171/171 [==============================] - 24s 137ms/step - loss: 1.4512\n",
      "Epoch 8/30\n",
      "171/171 [==============================] - 23s 137ms/step - loss: 1.4269\n",
      "Epoch 9/30\n",
      "171/171 [==============================] - 24s 139ms/step - loss: 1.4056\n",
      "Epoch 10/30\n",
      "171/171 [==============================] - 24s 139ms/step - loss: 1.3884\n",
      "Epoch 11/30\n",
      "171/171 [==============================] - 24s 138ms/step - loss: 1.3745\n",
      "Epoch 12/30\n",
      "171/171 [==============================] - 23s 137ms/step - loss: 1.3608\n",
      "Epoch 13/30\n",
      "171/171 [==============================] - 23s 136ms/step - loss: 1.3487\n",
      "Epoch 14/30\n",
      "171/171 [==============================] - 24s 140ms/step - loss: 1.3367\n",
      "Epoch 15/30\n",
      "171/171 [==============================] - 23s 136ms/step - loss: 1.3277\n",
      "Epoch 16/30\n",
      "171/171 [==============================] - 24s 142ms/step - loss: 1.3195\n",
      "Epoch 17/30\n",
      "171/171 [==============================] - 25s 144ms/step - loss: 1.3106\n",
      "Epoch 18/30\n",
      "171/171 [==============================] - 24s 140ms/step - loss: 1.3029\n",
      "Epoch 19/30\n",
      "171/171 [==============================] - 25s 146ms/step - loss: 1.2964\n",
      "Epoch 20/30\n",
      "171/171 [==============================] - 25s 144ms/step - loss: 1.2881\n",
      "Epoch 21/30\n",
      "171/171 [==============================] - 24s 140ms/step - loss: 1.2812\n",
      "Epoch 22/30\n",
      "171/171 [==============================] - 25s 144ms/step - loss: 1.2755\n",
      "Epoch 23/30\n",
      "171/171 [==============================] - 26s 154ms/step - loss: 1.2700\n",
      "Epoch 24/30\n",
      "171/171 [==============================] - 25s 144ms/step - loss: 1.2648\n",
      "Epoch 25/30\n",
      "171/171 [==============================] - 24s 140ms/step - loss: 1.2606\n",
      "Epoch 26/30\n",
      "171/171 [==============================] - 24s 142ms/step - loss: 1.2558\n",
      "Epoch 27/30\n",
      "171/171 [==============================] - 24s 139ms/step - loss: 1.2497\n",
      "Epoch 28/30\n",
      "171/171 [==============================] - 26s 152ms/step - loss: 1.2466\n",
      "Epoch 29/30\n",
      "171/171 [==============================] - 24s 143ms/step - loss: 1.2414\n",
      "Epoch 30/30\n",
      "171/171 [==============================] - 23s 136ms/step - loss: 1.2387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a5e978fe80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#perform the actually training/optimization.\n",
    "filepath=os.path.join(checkpoint_dir,\"weights-{epoch:02d}\")\n",
    "checkpoint = ModelCheckpoint(filepath,save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(dataset, epochs=30,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            8960      \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (1, None, 500)            1137000   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (1, None, 500)            0         \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (1, None, 500)            1503000   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (1, None, 500)            0         \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (1, None, 500)            1503000   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (1, None, 500)            0         \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (1, None, 500)            1503000   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (1, None, 500)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 35)             17535     \n",
      "=================================================================\n",
      "Total params: 5,672,495\n",
      "Trainable params: 5,672,495\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#rebuild the model with the weights, but modify it so it isn't expecting batches.\n",
    "model = get_model(1)\n",
    "model.load_weights(latest_checkpoint(checkpoint_dir))\n",
    "model.build(TensorShape([1, None]))\n",
    "model.summary()\n",
    "model.save(models_folder + model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "def generate_text(seed):\n",
    "    seed = seed.lower()\n",
    "    temperature = .01 #the temperature is used to skew the probabilities in a direction, to create more/less randomness in the output.\n",
    "    outputlen = 1000\n",
    "    ##This code is identical to how the web code works.\n",
    "    #setup all the maps that will be needed for converting to and from text to the model.\n",
    "    with open(models_folder + character_map) as json_file:\n",
    "        int_to_char = json.load(json_file)\n",
    "    char_to_int = { v : float(i) for (i, v) in enumerate(int_to_char)}#create a reverse map, since we'll have to conver their input.\n",
    "    #print(int_to_char)\n",
    "    #print(char_to_int)\n",
    "    n_vocab = len(int_to_char)#the number of characters in the vocabulary\n",
    "\n",
    "    #load the lstm model from our model file.\n",
    "    model = tf.keras.models.load_model(models_folder + model_file, compile=False)\n",
    "    input_text = [char_to_int[c] for c in seed]\n",
    "    input_text = tf.expand_dims(input_text,0)\n",
    "    output_text = []\n",
    "    model.reset_states()\n",
    "    for i in range(outputlen):\n",
    "        #run the input through our model.\n",
    "        predictions = model(input_text)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        #print(predictions)\n",
    "        predictions = predictions  / temperature #we devide the predictions by our temparature. For higher temperatures inject more randomness into the text.\n",
    "        \n",
    "        #select the prediction randomly, by sampling according to the prediction confidence.\n",
    "        predicted_int = tf.random.categorical(predictions,num_samples=1)[-1,0].numpy()\n",
    "        #predicted_int = np.argmax(predictions)\n",
    "        #print(predicted_int)\n",
    "        #pass forward to next stage\n",
    "        input_text = tf.expand_dims([predicted_int], 0)\n",
    "        output_text.append(int_to_char[predicted_int])\n",
    "    return (seed + ''.join(output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i don't think that thou hast said\n",
      "that thou shalt not be so but that the tower,\n",
      "and then the sun shall be the first company.\n",
      "\n",
      "king richard ii:\n",
      "then i shall not speak a word with the tower.\n",
      "\n",
      "king richard ii:\n",
      "then i shall not speak a word with the tower.\n",
      "\n",
      "king richard ii:\n",
      "then i shall not speak a word with the tower.\n",
      "\n",
      "king richard ii:\n",
      "then i shall not speak a word with the tower.\n",
      "\n",
      "king richard ii:\n",
      "then i shall not speak a word with the tower.\n",
      "\n",
      "king richard ii:\n",
      "then i shall not speak a word with the tower.\n",
      "\n",
      "king richard ii:\n",
      "then i shall not speak a word with the tower.\n",
      "\n",
      "king richard ii:\n",
      "then i shall not speak a word with the tower.\n",
      "\n",
      "king richard ii:\n",
      "then i shall not speak a word with the tower.\n",
      "\n",
      "king richard ii:\n",
      "then i shall not speak a word with the tower.\n",
      "\n",
      "king richard ii:\n",
      "then i shall not speak a word with the tower.\n",
      "\n",
      "king richard ii:\n",
      "then i shall not speak a word with the tower.\n",
      "\n",
      "king richard ii:\n",
      "then i shall not speak a word with the tower.\n",
      "\n",
      "king richard ii:\n",
      "then i shall not speak a word with the towe\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(seed=u\"I don't think that\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
