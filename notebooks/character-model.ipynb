{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.train import latest_checkpoint\n",
    "from tensorflow import TensorShape\n",
    "\n",
    "import json\n",
    "models_folder = \"../textgeneration/frontend/models/character-new/\"#the folder that the model information is saved in\n",
    "checkpoint_dir=\"./checkpoints\"#save checkpoints, so if things are interrupted we still have a result\n",
    "token_map = \"token-map.json\"#the token file\n",
    "model_file = \"model.h5\"#the model file\n",
    "\n",
    "seq_length = 100 #the size of sequence to use in training the model\n",
    "batch_size = 64 #number of batches to train with. Note that this was being used to train on a GPU. It may not work as well on a CPU\n",
    "embedding_dim = 256 #the dimension of the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "#Uncomment if using local corpus\n",
    "#discordf = \"../messages/discord-messages.txt\"\n",
    "#discord = open(discordf, 'r', encoding='utf-8').read()\n",
    "#fbf = \"../messages/facebook-messages.txt\"\n",
    "#fb = open(fbf, 'r', encoding='utf-8').read()\n",
    "#essayf = \"../messages/essays.txt\"\n",
    "#essay = open(essayf, 'r', encoding='utf-8').read()\n",
    "\n",
    "#merge the text of all three files together\n",
    "#raw_text = discord.lower() + \"\\n\" + fb.lower() + \"\\n\" + essay.lower()\n",
    "\n",
    "#load the shakespeare text corpus.\n",
    "shakespeare = open(\"../messages/shakespeare.txt\", 'r', encoding='utf-8').read()\n",
    "\n",
    "#cleanup the text a bit,\n",
    "raw_text = shakespeare.lower()\n",
    "raw_text = raw_text.encode(\"ascii\", \"ignore\").decode()#remove any non ascii characters.\n",
    "raw_text = re.sub(r\"[~#$%&*+;<=>\\[\\\\^_\\]`{|}0-9@/]\",\"\",raw_text)#strip out some ascii characters that aren't super important (reduces the vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  1111735\n",
      "Total Vocab:  35\n"
     ]
    }
   ],
   "source": [
    "# create mapping of unique chararacters to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = np.array(chars)\n",
    "\n",
    "#convert the entire corpus to be integers using our mapping\n",
    "text_as_int = np.array([char_to_int[c] for c in raw_text])\n",
    "\n",
    "#save our character mapping, since we need it to actually use the model\n",
    "with open(models_folder + token_map, 'w') as outfile:\n",
    "    json.dump(int_to_char.tolist(), outfile)\n",
    "    \n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "#Converts to a TF Dataset class, and creates a dataset containing the characters split into sequences\n",
    "#of length seq_length + 1\n",
    "char_dataset = Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "#For every sequence, we create our actual dataset by turning them into an input that is the original\n",
    "#sequence, and target sequence is the sequence shifted over one character.\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "#we randomize the dataset to help with training, then convert it into batches of the sequences.\n",
    "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           8960      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 500)           1137000   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (64, None, 500)           0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (64, None, 500)           1503000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (64, None, 500)           0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (64, None, 500)           1503000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (64, None, 500)           0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (64, None, 500)           1503000   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (64, None, 500)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 35)            17535     \n",
      "=================================================================\n",
      "Total params: 5,672,495\n",
      "Trainable params: 5,672,495\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#we create this function to retrieve our model, because it allows us to rebuild the model quickly, which we utilize later to\n",
    "#help with the fact that the model outputs sequences but we actually want a single character.\n",
    "def get_model(batch_size):\n",
    "    return Sequential([Embedding(n_vocab, embedding_dim,  batch_input_shape=[batch_size, None]),\n",
    "                  GRU(500, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  GRU(500, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  GRU(500, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  GRU(500, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  Dense(n_vocab)])\n",
    "#retrieve our model\n",
    "model = get_model(batch_size)\n",
    "#tweak our loss function, because we aren't doing a softmax on our dense layer here.\n",
    "def loss(labels, logits):\n",
    "    return sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "model.compile(loss=loss, optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "171/171 [==============================] - 22s 127ms/step - loss: 2.7638\n",
      "Epoch 2/30\n",
      "171/171 [==============================] - 22s 127ms/step - loss: 2.0109\n",
      "Epoch 3/30\n",
      "171/171 [==============================] - 22s 128ms/step - loss: 1.7129\n",
      "Epoch 4/30\n",
      "171/171 [==============================] - 22s 127ms/step - loss: 1.5908\n",
      "Epoch 5/30\n",
      "171/171 [==============================] - 22s 128ms/step - loss: 1.5261\n",
      "Epoch 6/30\n",
      "171/171 [==============================] - 22s 127ms/step - loss: 1.4838\n",
      "Epoch 7/30\n",
      "171/171 [==============================] - 22s 127ms/step - loss: 1.4546\n",
      "Epoch 8/30\n",
      "171/171 [==============================] - 22s 128ms/step - loss: 1.4276\n",
      "Epoch 9/30\n",
      "171/171 [==============================] - 22s 127ms/step - loss: 1.4080\n",
      "Epoch 10/30\n",
      "171/171 [==============================] - 22s 128ms/step - loss: 1.3907\n",
      "Epoch 11/30\n",
      "171/171 [==============================] - 22s 128ms/step - loss: 1.3751\n",
      "Epoch 12/30\n",
      "171/171 [==============================] - 22s 127ms/step - loss: 1.3628\n",
      "Epoch 13/30\n",
      "171/171 [==============================] - 22s 127ms/step - loss: 1.3505\n",
      "Epoch 14/30\n",
      "171/171 [==============================] - 22s 128ms/step - loss: 1.3388\n",
      "Epoch 15/30\n",
      "171/171 [==============================] - 22s 128ms/step - loss: 1.3305\n",
      "Epoch 16/30\n",
      "171/171 [==============================] - 22s 127ms/step - loss: 1.3201\n",
      "Epoch 17/30\n",
      "171/171 [==============================] - 22s 127ms/step - loss: 1.3123\n",
      "Epoch 18/30\n",
      "171/171 [==============================] - 22s 127ms/step - loss: 1.3041\n",
      "Epoch 19/30\n",
      "171/171 [==============================] - 22s 128ms/step - loss: 1.2961\n",
      "Epoch 20/30\n",
      "171/171 [==============================] - 22s 127ms/step - loss: 1.2896\n",
      "Epoch 21/30\n",
      "171/171 [==============================] - 22s 128ms/step - loss: 1.2838\n",
      "Epoch 22/30\n",
      "171/171 [==============================] - 22s 128ms/step - loss: 1.2762\n",
      "Epoch 23/30\n",
      "171/171 [==============================] - 22s 127ms/step - loss: 1.2720\n",
      "Epoch 24/30\n",
      "171/171 [==============================] - 22s 128ms/step - loss: 1.2657\n",
      "Epoch 25/30\n",
      "171/171 [==============================] - 22s 127ms/step - loss: 1.2598\n",
      "Epoch 26/30\n",
      "171/171 [==============================] - 22s 127ms/step - loss: 1.2553\n",
      "Epoch 27/30\n",
      "171/171 [==============================] - 22s 127ms/step - loss: 1.2509\n",
      "Epoch 28/30\n",
      "171/171 [==============================] - 22s 127ms/step - loss: 1.2467\n",
      "Epoch 29/30\n",
      "171/171 [==============================] - 22s 129ms/step - loss: 1.2418\n",
      "Epoch 30/30\n",
      "171/171 [==============================] - 22s 129ms/step - loss: 1.2382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ad4f102d00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup checkpoints\n",
    "filepath=os.path.join(checkpoint_dir,\"weights-{epoch:02d}\")\n",
    "checkpoint = ModelCheckpoint(filepath,save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "# Train the actual model.\n",
    "model.fit(dataset, epochs=30,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebuild the model with the weights, but modify it so it isn't expecting batches anymore.\n",
    "model = get_model(1)\n",
    "model.load_weights(latest_checkpoint(checkpoint_dir))\n",
    "model.build(TensorShape([1, None]))\n",
    "#we save this as our actual model, because it is not in a format where it is usable.\n",
    "model.save(models_folder + model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#generate function that we can run to generate text based on a seed value. This is very similar to the code that the page uses for generation\n",
    "def generate_text(seed):\n",
    "    seed = seed.lower()\n",
    "    temperature = .4 #the temperature is used to skew the probabilities in a direction, to create more/less randomness in the output.\n",
    "    outputlen = 1000# how long our output sequence is\n",
    "    \n",
    "    #load our token mapping\n",
    "    with open(models_folder + token_map) as json_file:\n",
    "        int_to_char = json.load(json_file)\n",
    "    char_to_int = { v : float(i) for (i, v) in enumerate(int_to_char)} #create a reverse map, since we'll have to conver their input.\n",
    "    n_vocab = len(int_to_char) #the number of characters in the vocabulary\n",
    "\n",
    "    #load the model from our model file.\n",
    "    model = tf.keras.models.load_model(models_folder + model_file, compile=False)\n",
    "    #convert the input text into integer values\n",
    "    input_text = [char_to_int[c] for c in seed]\n",
    "    input_text = tf.expand_dims(input_text,0)\n",
    "    \n",
    "    output_text = []\n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(outputlen):\n",
    "        #run the input through our model.\n",
    "        predictions = model(input_text)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions  / temperature #we devide the predictions by our temparature. For higher temperatures inject more randomness into the text.\n",
    "        \n",
    "        #select the prediction randomly, by sampling according to the prediction confidence.\n",
    "        predicted_int = tf.random.categorical(predictions,num_samples=1)[-1,0].numpy()\n",
    "        input_text = tf.expand_dims([predicted_int], 0)\n",
    "        output_text.append(int_to_char[predicted_int])\n",
    "    return (seed + ''.join(output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i don't think that i have left all that i should be deserved\n",
      "which now i can not be a princely sea the store,\n",
      "or else he would have seen the crown, and that is not so dear a place.\n",
      "\n",
      "lucio:\n",
      "i am not in the state and courteous time\n",
      "that he was not the souls of heaven is true.\n",
      "\n",
      "lucentio:\n",
      "i have said 'would they shall be so seem, when i am sorry,\n",
      "see the dead man that may be made you so with a season all this is the world.\n",
      "\n",
      "king richard ii:\n",
      "thou didst resign the duke of norfolk, then.\n",
      "\n",
      "buckingham:\n",
      "what would you tell me what mean that would be conscience?\n",
      "\n",
      "lucio:\n",
      "here's a widow and her son of sorrow,\n",
      "when he would be this and a brother's love,\n",
      "in the common prince and means to prison.\n",
      "\n",
      "clown:\n",
      "in god's name, i am so far to speak a king.\n",
      "\n",
      "gloucester:\n",
      "and that i was more a thing that i am too for the first\n",
      "can play the subjects for the beauty of the present court\n",
      "and hear me speak at our fortune made me speak.\n",
      "\n",
      "baptista:\n",
      "ay, and be honest with that the people,\n",
      "the thing i have forgot they shall be absent.\n",
      "\n",
      "kin\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(seed=u\"I don't think that\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
