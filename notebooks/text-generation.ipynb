{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "import json\n",
    "models_folder = \"../textgeneration/frontend/models/\"#the folder that the model information is stored within\n",
    "checkpoint_dir=\"./checkpoints\"\n",
    "#eventually, change these so specific model can be received in.\n",
    "character_map = \"character-map.json\"\n",
    "model_file = \"model.h5\"\n",
    "#the length of the input sequences to be fed through the network\n",
    "seq_length = 100\n",
    "batch_size = 64\n",
    "embedding_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "discordf = \"../messages/discord-messages.txt\"\n",
    "discord = open(discordf, 'r', encoding='utf-8').read()\n",
    "fbf = \"../messages/facebook-messages.txt\"\n",
    "fb = open(fbf, 'r', encoding='utf-8').read()\n",
    "essayf = \"../messages/essays.txt\"\n",
    "essay = open(essayf, 'r', encoding='utf-8').read()\n",
    "\n",
    "#cleanup the text a bit,\n",
    "raw_text = discord.lower() + \"\\n\" + fb.lower() + \"\\n\" + essay.lower()\n",
    "raw_text = raw_text.encode(\"ascii\", \"ignore\").decode()#remove any non ascii characters.\n",
    "raw_text = re.sub(r\"[~#$%&*+;<=>\\[\\\\^_\\]`{|}0-9@/]\",\"\",raw_text)#strip out some ascii characters that aren't super important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  1547827\n",
      "Total Vocab:  70\n"
     ]
    }
   ],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = np.array(chars)\n",
    "text_as_int = np.array([char_to_int[c] for c in raw_text])\n",
    "#print(char_to_int)\n",
    "#print(int_to_char)\n",
    "\n",
    "#save our character mapping, since we need it to actually use the model\n",
    "with open(models_folder + character_map, 'w') as outfile:\n",
    "    json.dump(int_to_char.tolist(), outfile)\n",
    "    \n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "#Cut the text into sequences\n",
    "char_dataset = Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "#The input is a sequence of seq_lenght, and the output is the same sequence shifted to reveal\n",
    "#an additional letter.\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_25 (Embedding)     (64, None, 256)           17920     \n",
      "_________________________________________________________________\n",
      "gru_46 (GRU)                 (64, None, 500)           1137000   \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (64, None, 500)           0         \n",
      "_________________________________________________________________\n",
      "gru_47 (GRU)                 (64, None, 500)           1503000   \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (64, None, 500)           0         \n",
      "_________________________________________________________________\n",
      "gru_48 (GRU)                 (64, None, 500)           1503000   \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (64, None, 500)           0         \n",
      "_________________________________________________________________\n",
      "gru_49 (GRU)                 (64, None, 500)           1503000   \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (64, None, 500)           0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (64, None, 70)            35070     \n",
      "=================================================================\n",
      "Total params: 5,698,990\n",
      "Trainable params: 5,698,990\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model(batch_size):\n",
    "    return Sequential([Embedding(n_vocab, embedding_dim,  batch_input_shape=[batch_size, None]),\n",
    "                  GRU(500, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  GRU(500, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  GRU(500, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  GRU(500, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                  Dropout(.2),\n",
    "                  Dense(n_vocab)])\n",
    "# define the LSTM model\n",
    "model = get_model(batch_size)\n",
    "#model.add(LSTM(300, return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(LSTM(300, return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(LSTM(300, return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "def loss(labels, logits):\n",
    "    return sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "model.compile(loss=loss, optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "239/239 [==============================] - 20s 85ms/step - loss: 2.4928\n",
      "Epoch 2/30\n",
      "239/239 [==============================] - 20s 83ms/step - loss: 1.7737\n",
      "Epoch 3/30\n",
      "239/239 [==============================] - 21s 87ms/step - loss: 1.6180\n",
      "Epoch 4/30\n",
      "239/239 [==============================] - 21s 87ms/step - loss: 1.5462\n",
      "Epoch 5/30\n",
      "239/239 [==============================] - 21s 87ms/step - loss: 1.5017\n",
      "Epoch 6/30\n",
      "239/239 [==============================] - 21s 87ms/step - loss: 1.4711\n",
      "Epoch 7/30\n",
      "239/239 [==============================] - 33s 140ms/step - loss: 1.4461\n",
      "Epoch 8/30\n",
      "239/239 [==============================] - 22s 91ms/step - loss: 1.4264\n",
      "Epoch 9/30\n",
      "239/239 [==============================] - 27s 114ms/step - loss: 1.4111\n",
      "Epoch 10/30\n",
      "239/239 [==============================] - 20s 83ms/step - loss: 1.3975\n",
      "Epoch 11/30\n",
      "239/239 [==============================] - 29s 120ms/step - loss: 1.3864\n",
      "Epoch 12/30\n",
      "239/239 [==============================] - 21s 86ms/step - loss: 1.3761\n",
      "Epoch 13/30\n",
      "239/239 [==============================] - 21s 88ms/step - loss: 1.3668\n",
      "Epoch 14/30\n",
      "239/239 [==============================] - 20s 82ms/step - loss: 1.3594\n",
      "Epoch 15/30\n",
      "239/239 [==============================] - 19s 81ms/step - loss: 1.3522\n",
      "Epoch 16/30\n",
      "239/239 [==============================] - 20s 83ms/step - loss: 1.3454\n",
      "Epoch 17/30\n",
      "239/239 [==============================] - 21s 86ms/step - loss: 1.3403\n",
      "Epoch 18/30\n",
      "239/239 [==============================] - 20s 83ms/step - loss: 1.3350\n",
      "Epoch 19/30\n",
      "239/239 [==============================] - 20s 83ms/step - loss: 1.3304\n",
      "Epoch 20/30\n",
      "239/239 [==============================] - 19s 81ms/step - loss: 1.3264\n",
      "Epoch 21/30\n",
      "239/239 [==============================] - 21s 87ms/step - loss: 1.3222\n",
      "Epoch 22/30\n",
      "239/239 [==============================] - 21s 86ms/step - loss: 1.3183\n",
      "Epoch 23/30\n",
      "239/239 [==============================] - 21s 88ms/step - loss: 1.3154\n",
      "Epoch 24/30\n",
      "239/239 [==============================] - 20s 85ms/step - loss: 1.3130\n",
      "Epoch 25/30\n",
      "239/239 [==============================] - 21s 89ms/step - loss: 1.3099\n",
      "Epoch 26/30\n",
      "239/239 [==============================] - 22s 91ms/step - loss: 1.3076\n",
      "Epoch 27/30\n",
      "239/239 [==============================] - 22s 93ms/step - loss: 1.3051\n",
      "Epoch 28/30\n",
      "239/239 [==============================] - 21s 87ms/step - loss: 1.3037\n",
      "Epoch 29/30\n",
      "239/239 [==============================] - 20s 85ms/step - loss: 1.3009\n",
      "Epoch 30/30\n",
      "239/239 [==============================] - 20s 83ms/step - loss: 1.3007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x10acf9c4f40>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#perform the actually training/optimization.\n",
    "filepath=os.path.join(checkpoint_dir,\"weights-{epoch:02d}\")\n",
    "checkpoint = ModelCheckpoint(filepath,save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(dataset, epochs=30,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     (1, None, 256)            17920     \n",
      "_________________________________________________________________\n",
      "gru_50 (GRU)                 (1, None, 500)            1137000   \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (1, None, 500)            0         \n",
      "_________________________________________________________________\n",
      "gru_51 (GRU)                 (1, None, 500)            1503000   \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (1, None, 500)            0         \n",
      "_________________________________________________________________\n",
      "gru_52 (GRU)                 (1, None, 500)            1503000   \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (1, None, 500)            0         \n",
      "_________________________________________________________________\n",
      "gru_53 (GRU)                 (1, None, 500)            1503000   \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (1, None, 500)            0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (1, None, 70)             35070     \n",
      "=================================================================\n",
      "Total params: 5,698,990\n",
      "Trainable params: 5,698,990\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#rebuild the model with the weights, but modify it so it isn't expecting batches.\n",
    "model = get_model(1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()\n",
    "model.save(models_folder + model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "def generate_text(seed):\n",
    "    seed = seed.lower()\n",
    "    temperature = .5 #the temperature is used to skew the probabilities in a direction, to create more/less randomness in the output.\n",
    "    outputlen = 1000\n",
    "    ##This code is identical to how the web code works.\n",
    "    #setup all the maps that will be needed for converting to and from text to the model.\n",
    "    with open(models_folder + character_map) as json_file:\n",
    "        int_to_char = json.load(json_file)\n",
    "    char_to_int = { v : float(i) for (i, v) in enumerate(int_to_char)}#create a reverse map, since we'll have to conver their input.\n",
    "    #print(int_to_char)\n",
    "    #print(char_to_int)\n",
    "    n_vocab = len(int_to_char)#the number of characters in the vocabulary\n",
    "\n",
    "    #load the lstm model from our model file.\n",
    "    model = tf.keras.models.load_model(models_folder + model_file, compile=False)\n",
    "    input_text = [char_to_int[c] for c in seed]\n",
    "    input_text = tf.expand_dims(input_text,0)\n",
    "    output_text = []\n",
    "    model.reset_states()\n",
    "    for i in range(outputlen):\n",
    "        #run the input through our model.\n",
    "        predictions = model(input_text)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        #print(predictions)\n",
    "        predictions = predictions  / temperature #we devide the predictions by our temparature. For higher temperatures inject more randomness into the text.\n",
    "        \n",
    "        #select the prediction randomly, by sampling according to the prediction confidence.\n",
    "        predicted_int = tf.random.categorical(predictions,num_samples=1)[-1,0].numpy()\n",
    "        #predicted_int = np.argmax(predictions)\n",
    "        #print(predicted_int)\n",
    "        #pass forward to next stage\n",
    "        input_text = tf.expand_dims([predicted_int], 0)\n",
    "        output_text.append(int_to_char[predicted_int])\n",
    "    return (seed + ''.join(output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i don't think that's what we all are all approaching the show so much more for the bottom with an assumption that other things are all a company and then we take it to be closely done as a massive file, which is a lot of people who have to do with the side of secure deletion of cases, so i don't think it was a lot to play the really like that\n",
      "there are significant wealth and profit more people are actually on the problem of the standard for the thing, but that seems to be a straight up for the ball of the prize of it and then look fine. i think i can totally like the first problem though, the next server that the device is more of a place in the same price of potential and the last sense of deep how another distribution that it was a different environment of called part of the character is wrong, but it is a silicon valley though, even if it is a second regular state of company and an advanced and make me on the first one when they do the story is interesting. i think the first one was a bit of the mone\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(seed=u\"I don't think that\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}