{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy\n",
    "import re\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import json\n",
    "models_folder = \"../textgeneration/frontend/models/\"#the folder that the model information is stored within\n",
    "#eventually, change these so specific model can be received in.\n",
    "character_map = \"character-map.json\"\n",
    "model_file = \"model.h5\"\n",
    "outputlen = 1000\n",
    "temperature = 0.04 #the temperature is used to skew the probabilities in a direction, to create more/less randomness in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  1529921\n",
      "Total Vocab:  38\n",
      "Total Patterns:  1529621\n"
     ]
    }
   ],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "discordf = \"../messages/discord-messages.txt\"\n",
    "discord = open(discordf, 'r', encoding='utf-8').read()\n",
    "fbf = \"../messages/facebook-messages.txt\"\n",
    "fb = open(fbf, 'r', encoding='utf-8').read()\n",
    "essayf = \"../messages/essays.txt\"\n",
    "essay = open(essayf, 'r', encoding='utf-8').read()\n",
    "\n",
    "#cleanup the text a bit,\n",
    "raw_text = discord.lower() + \"\\n\" + fb.lower() + \"\\n\" + essay.lower()\n",
    "raw_text = raw_text.encode(\"ascii\", \"ignore\").decode()#remove any non ascii characters.\n",
    "raw_text = re.sub(r\"[~#$%&*+;<=>\\[\\\\^_\\]`{|}0-9@/]\",\"\",raw_text)#strip out some ascii characters thataren't super important.\n",
    "\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "#save our character mapping, since we need it to actually use the model\n",
    "with open(models_folder + character_map, 'w') as outfile:\n",
    "    json.dump(int_to_char, outfile)\n",
    "    \n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 300\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "5976/5976 [==============================] - ETA: 0s - loss: 2.3113\n",
      "Epoch 00001: loss improved from inf to 2.31134, saving model to checkpoints\\weights-01-2.3113-bigger.hdf5\n",
      "5976/5976 [==============================] - 2073s 347ms/step - loss: 2.3113\n",
      "Epoch 2/16\n",
      "5976/5976 [==============================] - ETA: 0s - loss: 1.7863\n",
      "Epoch 00002: loss improved from 2.31134 to 1.78626, saving model to checkpoints\\weights-02-1.7863-bigger.hdf5\n",
      "5976/5976 [==============================] - 2060s 345ms/step - loss: 1.7863\n",
      "Epoch 3/16\n",
      "5976/5976 [==============================] - ETA: 0s - loss: 1.6742\n",
      "Epoch 00003: loss improved from 1.78626 to 1.67422, saving model to checkpoints\\weights-03-1.6742-bigger.hdf5\n",
      "5976/5976 [==============================] - 2061s 345ms/step - loss: 1.6742\n",
      "Epoch 4/16\n",
      "5976/5976 [==============================] - ETA: 0s - loss: 1.6122\n",
      "Epoch 00004: loss improved from 1.67422 to 1.61223, saving model to checkpoints\\weights-04-1.6122-bigger.hdf5\n",
      "5976/5976 [==============================] - 2063s 345ms/step - loss: 1.6122\n",
      "Epoch 5/16\n",
      "5976/5976 [==============================] - ETA: 0s - loss: 1.5712\n",
      "Epoch 00005: loss improved from 1.61223 to 1.57119, saving model to checkpoints\\weights-05-1.5712-bigger.hdf5\n",
      "5976/5976 [==============================] - 2063s 345ms/step - loss: 1.5712\n",
      "Epoch 6/16\n",
      "5976/5976 [==============================] - ETA: 0s - loss: 1.5429\n",
      "Epoch 00006: loss improved from 1.57119 to 1.54286, saving model to checkpoints\\weights-06-1.5429-bigger.hdf5\n",
      "5976/5976 [==============================] - 2061s 345ms/step - loss: 1.5429\n",
      "Epoch 7/16\n",
      "5976/5976 [==============================] - ETA: 0s - loss: 1.5212\n",
      "Epoch 00007: loss improved from 1.54286 to 1.52125, saving model to checkpoints\\weights-07-1.5212-bigger.hdf5\n",
      "5976/5976 [==============================] - 2060s 345ms/step - loss: 1.5212\n",
      "Epoch 8/16\n",
      "5976/5976 [==============================] - ETA: 0s - loss: 1.5033\n",
      "Epoch 00008: loss improved from 1.52125 to 1.50334, saving model to checkpoints\\weights-08-1.5033-bigger.hdf5\n",
      "5976/5976 [==============================] - 2060s 345ms/step - loss: 1.5033\n",
      "Epoch 9/16\n",
      "5976/5976 [==============================] - ETA: 0s - loss: 1.4875\n",
      "Epoch 00009: loss improved from 1.50334 to 1.48755, saving model to checkpoints\\weights-09-1.4875-bigger.hdf5\n",
      "5976/5976 [==============================] - 2104s 352ms/step - loss: 1.4875\n",
      "Epoch 10/16\n",
      "5976/5976 [==============================] - ETA: 0s - loss: 1.4725\n",
      "Epoch 00010: loss improved from 1.48755 to 1.47247, saving model to checkpoints\\weights-10-1.4725-bigger.hdf5\n",
      "5976/5976 [==============================] - 2095s 351ms/step - loss: 1.4725\n",
      "Epoch 11/16\n",
      "5976/5976 [==============================] - ETA: 0s - loss: 1.4615\n",
      "Epoch 00011: loss improved from 1.47247 to 1.46151, saving model to checkpoints\\weights-11-1.4615-bigger.hdf5\n",
      "5976/5976 [==============================] - 2090s 350ms/step - loss: 1.4615\n",
      "Epoch 12/16\n",
      "5976/5976 [==============================] - ETA: 0s - loss: 1.4504\n",
      "Epoch 00012: loss improved from 1.46151 to 1.45038, saving model to checkpoints\\weights-12-1.4504-bigger.hdf5\n",
      "5976/5976 [==============================] - 2090s 350ms/step - loss: 1.4504\n",
      "Epoch 13/16\n",
      "5976/5976 [==============================] - ETA: 0s - loss: 1.4423\n",
      "Epoch 00013: loss improved from 1.45038 to 1.44232, saving model to checkpoints\\weights-13-1.4423-bigger.hdf5\n",
      "5976/5976 [==============================] - 2089s 350ms/step - loss: 1.4423\n",
      "Epoch 14/16\n",
      "5976/5976 [==============================] - ETA: 0s - loss: 1.4347\n",
      "Epoch 00014: loss improved from 1.44232 to 1.43468, saving model to checkpoints\\weights-14-1.4347-bigger.hdf5\n",
      "5976/5976 [==============================] - 2089s 350ms/step - loss: 1.4347\n",
      "Epoch 15/16\n",
      "5976/5976 [==============================] - ETA: 0s - loss: 1.4283\n",
      "Epoch 00015: loss improved from 1.43468 to 1.42830, saving model to checkpoints\\weights-15-1.4283-bigger.hdf5\n",
      "5976/5976 [==============================] - 2089s 350ms/step - loss: 1.4283\n",
      "Epoch 16/16\n",
      "5976/5976 [==============================] - ETA: 0s - loss: 1.4225\n",
      "Epoch 00016: loss improved from 1.42830 to 1.42251, saving model to checkpoints\\weights-16-1.4225-bigger.hdf5\n",
      "5976/5976 [==============================] - 2090s 350ms/step - loss: 1.4225\n"
     ]
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(300, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(300, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(300, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(300))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "#use checkpoints to keep trac of the current progress.\n",
    "filepath=\"checkpoints/weights-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=16, batch_size=256, callbacks=callbacks_list)\n",
    "model.save(models_folder + model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model = Sequential()\n",
    "model.add(LSTM(300, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(300, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(300, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(300))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.load_weights(\"checkpoints\\weights-13-1.4986-bigger.hdf5\")\n",
    "model.save('model-full.h5')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random seed to use for testing the generation\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "text = dataX[start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../textgeneration/frontend/models/basic/character-map.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-c964c43f4801>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#setup all the maps that will be needed for converting to and from text to the model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels_folder\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcharacter_map\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mint_to_char\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mint_to_char\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mint_to_char\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;31m#this is to fix the mapping so it has integer keys like it is supposed to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../textgeneration/frontend/models/basic/character-map.json'"
     ]
    }
   ],
   "source": [
    "##This code is identical to how the web code works.\n",
    "\n",
    "#setup all the maps that will be needed for converting to and from text to the model.\n",
    "with open(models_folder + character_map) as json_file:\n",
    "    int_to_char = json.load(json_file)\n",
    "int_to_char = { int(key) : value  for (key, value) in int_to_char.items()}#this is to fix the mapping so it has integer keys like it is supposed to\n",
    "char_to_int = { value : int(key) for (key, value) in int_to_char.items()}#create a reverse map, since we'll have to conver their input.\n",
    "vocab_size = len(int_to_char.keys())#the number of characters in the vocabulary\n",
    "\n",
    "\n",
    "#load the lstm model from our model file.\n",
    "model = tf.keras.models.load_model(models_folder + model_file)\n",
    "\n",
    "for i in range(outputlen):\n",
    "    #we convert text to be the correct shape for the lstm, and we squish all the values to be between 0 and 1\n",
    "    x = numpy.reshape(text, (1, len(text), 1))\n",
    "    x = x / float(vocab_size)\n",
    "\n",
    "    #run the input through our model.\n",
    "    predictions = model.predict(x, verbose=0)\n",
    "    predictions = predictions / temperature #we devide the predictions by our temparature. For higher temperatures inject more randomness into the text.\n",
    "    #select the prediction randomly, by sampling according to the prediction confidence.\n",
    "    predicted_char = tf.random.categorical(predictions,num_samples=1)[-1,0].numpy()\n",
    "\n",
    "    result = int_to_char[predicted_char]\n",
    "\n",
    "    #Add the character to our text, and bump out the first letter\n",
    "    text.append(predicted_char)\n",
    "    text = text[1:len(text)]\n",
    "    print(result, end = '',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
